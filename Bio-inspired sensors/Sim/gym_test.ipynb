{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the tactile gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 20 2023 16:26:58\n"
     ]
    }
   ],
   "source": [
    "from tactile_gym.rl_envs.demo_rl_env_base import demo_rl_env\n",
    "from tactile_gym.rl_envs.exploration.edge_follow.edge_follow_env import EdgeFollowEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up some variables of what we want within the envirnment. \n",
    "\n",
    "Number of iterations is how long our training loop will be\n",
    "\n",
    "max_steps is how many steps each iteration can ake within the environment\n",
    "\n",
    "show_gui allows us to view or not view a gui showing whats going on\n",
    "\n",
    "This is the same with show tactle\n",
    "\n",
    "Rendering is required for showing the gui - it is quicker without\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed = int(0)\n",
    "num_iter = 100\n",
    "max_steps = 250\n",
    "show_gui = True\n",
    "show_tactile = True\n",
    "render = True\n",
    "print_info = False\n",
    "image_size = [128, 128]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=3\n",
      "argv[0] = --unused\n",
      "argv[1] = \n",
      "argv[2] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Mesa/X.org\n",
      "GL_RENDERER=llvmpipe (LLVM 12.0.0, 256 bits)\n",
      "GL_VERSION=4.5 (Core Profile) Mesa 21.2.6\n",
      "GL_SHADING_LANGUAGE_VERSION=4.50\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.5 (Core Profile) Mesa 21.2.6\n",
      "Vendor = Mesa/X.org\n",
      "Renderer = llvmpipe (LLVM 12.0.0, 256 bits)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Mesa/X.org\n",
      "ven = Mesa/X.org\n"
     ]
    }
   ],
   "source": [
    "env_modes = {\n",
    "    # which dofs can have movement\n",
    "    \"movement_mode\": \"xy\",\n",
    "\n",
    "    # specify arm\n",
    "    \"arm_type\": \"ur5\",\n",
    "\n",
    "    # specify tactile sensor\n",
    "    \"tactile_sensor_name\": \"tactip\",\n",
    "    # \"tactile_sensor_name\": \"digit\",\n",
    "    # \"tactile_sensor_name\": \"digitac\",\n",
    "\n",
    "    # the type of control used\n",
    "    # \"control_mode\": \"TCP_position_control\",\n",
    "    'control_mode': 'TCP_velocity_control',\n",
    "\n",
    "    # add variation to embed distance to optimise for\n",
    "    # warning, don't use rand height when controlling z unless\n",
    "    # including embed distance in observation\n",
    "    # 'noise_mode':'fixed_height',\n",
    "    \"noise_mode\": \"rand_height\",\n",
    "\n",
    "    # which observation type to return\n",
    "    'observation_mode': 'oracle',\n",
    "    # \"observation_mode\": \"tactile\",\n",
    "    # 'observation_mode':'visual',\n",
    "    # 'observation_mode':'visuotactile',\n",
    "\n",
    "    # which reward type to use (currently only dense)\n",
    "    \"reward_mode\": \"dense\"\n",
    "    # 'reward_mode':'sparse'\n",
    "}\n",
    "env = EdgeFollowEnv(\n",
    "    max_steps=max_steps,\n",
    "    env_modes=env_modes,\n",
    "    show_gui=show_gui,\n",
    "    show_tactile=show_tactile,\n",
    "    image_size=image_size\n",
    ")\n",
    "\n",
    "# set seed for deterministic results\n",
    "env.seed(seed)\n",
    "env.action_space.np_random.seed(seed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate GUI is selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create controllable parameters on GUI\n",
    "action_ids = []\n",
    "min_action = env.min_action\n",
    "max_action = env.max_action\n",
    "if show_gui:\n",
    "\n",
    "    if env_modes[\"movement_mode\"] == \"xy\":\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dX\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dY\", min_action, max_action, 0))\n",
    "\n",
    "    elif env_modes[\"movement_mode\"] == \"xyz\":\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dX\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dY\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dZ\", min_action, max_action, 0))\n",
    "\n",
    "    elif env_modes[\"movement_mode\"] == \"xyRz\":\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dX\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dY\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dRz\", min_action, max_action, 0))\n",
    "\n",
    "    elif env_modes[\"movement_mode\"] == \"xyzRz\":\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dX\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dY\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dZ\", min_action, max_action, 0))\n",
    "        action_ids.append(env._pb.addUserDebugParameter(\"dRz\", min_action, max_action, 0))\n",
    "\n",
    "# run the control loop\n",
    "#demo_rl_env(env, num_iter, action_ids, show_gui, show_tactile, render, print_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward:  -43.75193943579056\n",
      "Total Reward:  -43.752371808237626\n",
      "Total Reward:  -43.75191886884572\n",
      "Total Reward:  -43.759206173897205\n",
      "Total Reward:  -43.74977846050147\n",
      "Total Reward:  -43.75073885920083\n",
      "Total Reward:  -43.75706046836393\n",
      "Total Reward:  -43.75177311475714\n",
      "Total Reward:  -43.75836015026896\n",
      "Total Reward:  -43.75127830115847\n",
      "Total Reward:  -43.7501868941185\n",
      "Total Reward:  -43.76027131257435\n",
      "Total Reward:  -43.7516513259499\n",
      "Total Reward:  -43.75254139180789\n",
      "Total Reward:  -43.761771379095656\n",
      "Total Reward:  -43.7544636385033\n",
      "Total Reward:  -43.758304640054675\n",
      "Total Reward:  -43.75660863241039\n",
      "Total Reward:  -43.75305294734375\n",
      "Total Reward:  -43.761797928391616\n",
      "Total Reward:  -43.75515286873353\n",
      "Total Reward:  -43.75357542150951\n",
      "Total Reward:  -43.75320094554295\n",
      "Total Reward:  -43.75048256419961\n",
      "Total Reward:  -43.75194146199684\n",
      "Total Reward:  -43.75264700208222\n",
      "Total Reward:  -43.752768846287545\n",
      "Total Reward:  -43.754573341859164\n",
      "Total Reward:  -43.75276890965002\n",
      "Total Reward:  -43.750782981075346\n",
      "Total Reward:  -43.751375562627786\n",
      "Total Reward:  -43.750978286660505\n",
      "Total Reward:  -43.75255725266318\n",
      "Total Reward:  -43.751636052180004\n",
      "Total Reward:  -43.75066439335902\n",
      "Total Reward:  -43.75300223832217\n",
      "Total Reward:  -43.75201303887075\n",
      "Total Reward:  -43.75086782114984\n",
      "Total Reward:  -43.75109953959175\n",
      "Total Reward:  -43.7509381917613\n",
      "Total Reward:  -43.75475590724\n",
      "Total Reward:  -43.75215477848599\n",
      "Total Reward:  -43.751049124116776\n",
      "Total Reward:  -43.76143390792638\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# collection loop\n",
    "for i in range(num_iter):\n",
    "    r_sum = 0\n",
    "    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "    step = 0\n",
    "\n",
    "    while not d:\n",
    "\n",
    "        if show_gui:\n",
    "            a = []\n",
    "            for action_id in action_ids:\n",
    "                a.append(env._pb.readUserDebugParameter(action_id))\n",
    "        else:\n",
    "            a = env.action_space.sample()\n",
    "\n",
    "        # step the environment\n",
    "        o, r, d, info = env.step(a)\n",
    "\n",
    "        if print_info:\n",
    "            print(\"\")\n",
    "            print(\"Step: \", step)\n",
    "            print(\"Act:  \", a)\n",
    "            print(\"Obs:  \")\n",
    "            for key, value in o.items():\n",
    "                if value is None:\n",
    "                    print(\"  \", key, \":\", value)\n",
    "                else:\n",
    "                    print(\"  \", key, \":\", value.shape)\n",
    "            print(\"Rew:  \", r)\n",
    "            print(\"Done: \", d)\n",
    "        # render visual + tactile observation\n",
    "        if render:\n",
    "            render_img = env.render()\n",
    "\n",
    "        r_sum += r\n",
    "        step += 1\n",
    "\n",
    "        q_key = ord(\"q\")\n",
    "        r_key = ord(\"r\")\n",
    "        keys = env._pb.getKeyboardEvents()\n",
    "        if q_key in keys and keys[q_key] & env._pb.KEY_WAS_TRIGGERED:\n",
    "            exit()\n",
    "        elif r_key in keys and keys[r_key] & env._pb.KEY_WAS_TRIGGERED:\n",
    "            d = True\n",
    "\n",
    "    print(\"Total Reward: \", r_sum)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
